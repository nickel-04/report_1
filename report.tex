\documentclass{article}
\usepackage[english]{babel}
\usepackage{authblk,graphicx,float}
\usepackage{amssymb}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\title{Project 3 Proposal  \\ \ \\IN9310 - Advanced Deep Learning for Image Analysis
}
\author{Kumar, Nikhil}

\date{\today}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Project: Utilizing the Variational Bayes Autoencoder in Survival Analysis for 3D CT data}


\section{Introduction}
\subsection{Problem definition}
In medical research, particularly in survival analysis, researchers face the challenge of handling complex, varied, incomplete, and censored data. Censored data occurs when the event under study (like a patient's death) hasn't occurred for various reasons—such as a patient dropping out of the study, switching hospitals, or the study ending while the patient is still alive. Traditional methods like regression analysis cannot be used effectively because they assume that the observed survival time is the actual time until the event, which is not true for censored data.   

Accurate predictions in survival analysis are vital. They allow healthcare professionals to better assess treatment effectiveness, manage disease progression, and enhance patient care. Mistakes due to misunderstood data can lead to incorrect medical guidance, affecting patient outcomes.

Historically, survival analysis has predominantly been conducted on clinical datasets, with models developed specifically for these types of data. Most benchmark datasets in this field are clinical, which has limited the diversity of models. Although there are survival analysis models that use 3D images, they typically employ simple encoder-decoder architectures without leveraging the complexities of the data. These approaches lack robustness and flexibility because they do not incorporate advanced concepts like latent variables, which are crucial for capturing deeper, more nuanced patterns in the data. 

The project introduces a novel method, the Survival Analysis Variational Autoencoder (SAVAE), that innovatively uses Variational Autoencoders (VAEs) designed for survival analysis. SAVAE addresses censored data challenges by employing a special formula and allowing for dynamic probability calculations. This method is superior to traditional survival models as it leverages latent variables for greater robustness and predictive accuracy. It has demonstrated strong and stable results across various measures on clinical datasets.

Currently, SAVAE is implemented for clinical data; however, there is an opportunity to extend this model to 3D image data. Such an expansion would allow the exploration of more complex medical scenarios where 3D imaging plays a critical role, such as in oncology or neurology, providing a more comprehensive tool for medical research and patient care.

By expanding SAVAE to work with both traditional clinical and 3D image data, we're not just improving how we handle censored data; we're potentially setting a new standard in the field. It also helps with other tasks like grouping similar patients, filling in missing information, and even creating realistic patient simulations for further study.


\subsection{Contributions}
The central contribution of this project is adapting the existing SAVAE model, originally developed for clinical datasets, to handle 3D image data. This adaptation involves modifying the model’s architecture to effectively process the complexities and characteristics of 3D imaging, pivotal in medical fields like radiology and oncology. Moreover, the Evidence Lower Bound (ELBO), a key component in training Variational Autoencoders, has been customized to better suit the unique demands of survival analysis with 3D imaging. By doing so, the project aims to extend the utility of survival analysis into new areas that rely heavily on detailed imaging for diagnostics and prognosis, while using established methods to handle censored data effectively within these new contexts.

\section{Related work}
\subsection{Literature}

\subsection{Contrast with the project}

Recent advances in medical research have shifted towards using Deep Learning (DL) to predict important health events like disease progress and patient survival. Although traditional statistical methods like the Cox proportional hazards model \cite{kraisangka2018bayesian} are commonly used, they rely on assumptions that may not hold true with complex data, such as constant hazard functions and linear relationships without interactions among variables. 

As a response, researchers have started using Deep Neural Networks (DNNs) to handle non-linear and complex relationships in data. Models like DeepSurv \cite{jared2016deep} have tried to adapt older models to capture these complexities but still assume proportional hazards. A new model, DeepHit \cite{lee2018deephit}, moves away from this assumption and has shown promising results, although it requires large datasets. 

To address these issues, I introduce a new model called SAVAE (Survival Analysis Variational Autoencoder). SAVAE uses a flexible, generative approach with Variational Autoencoders to predict event timings more accurately and can handle different types of data, including censored data from patients who haven't yet experienced the event of interest. This makes it a versatile tool in medical research, promising better predictions and adaptability across various domains.

\section{Proposal}
\subsection{Method}

\section{Results}
\subsection{Experimental protocol}
\subsection{Experimental results}
\subsection{Experimental discussion}

\section{Methods}
\subsection{Dataset}
In survival analysis (SA), each patient's data shows if and when certain events happened during a study, noting which data is complete and which is not (censored). The SAVAE model was tested using ten different disease datasets, such as studies on heart attacks, serious illnesses, breast cancer, children's cancer, blood disorders, liver disease,infant pneumonia, and colorectal liver metastases. These datasets vary in the type of data they include, like clinical details, demographics, and specific disease markers. The choice of these datasets was deliberate to ensure a broad test of the model across different diseases and patient information types. This varied selection helps to assess how well the model performs in real-world scenarios, considering datasets with different amounts of incomplete data and various lengths of time until an event occurs.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{figures/Dataset.png}
    \caption{Data information from datasets used to train SAVAE model. I have analyzed ten different disease datasets. Each contains different patient information such as genomic, clinical, or demographic data. }
    \label{Dataset}

\end{figure}

\subsection{VAE}
Introduced in 2013 \cite{kingma2013auto}, the Variational Autoencoder (VAE) utilizes deep neural networks for Bayesian inference on datasets where all observations \(x_i\) are independent given the latent variable \(z\). The model generates data \(x_i\) from a conditional distribution \(p_\theta(x|z)\), streamlining the Bayesian inference process. To handle the computational challenges of deriving the true posterior \(p_\theta(z|x)\), VAE employs a variational approach with a family \(q_\phi(z|x)\), optimizing parameters \(\phi\) to enhance approximation quality. To ensure the objective function is differentiable, the reparametrization trick is used. Initially, sampling directly from the encoder outputs \(\mu\) and \(\sigma\) made the process non-deterministic. To make it deterministic,  I sample from a standard normal distribution and adjust by multiplying by \(\sigma\) and adding \(\mu\), thereby enabling gradient-based optimization techniques.

\begin{figure}[H]
    \centering
    \includegraphics[width=4cm]{figures/VAE.png}
    \caption{Bayesian VAE vanilla model. The shaded circle refers to the latent variable z, and the white circle refers to the observable x. Probabilities \(p_\theta(x|z)\) and \(q_\phi(z|x)\) denote, respectively, the generative model and the variational approximation to the posterior, since the true posterior p(z|x) is unknown.}
    \label{VAE}
\end{figure}

The ELBO of the VAE is given by:

\begin{equation}
    \log p_\theta(x) \geq -D_{KL}(q_\phi(z|x) \parallel p(z)) + \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] = L(x, \theta, \phi),
\end{equation}

Instead of directly maximizing \(\log p_\theta(x)\), I focus on maximizing the lower bound on the right-hand side of Equation 1, which includes the Kullback-Leibler divergence and the log likelihood.

\subsection{Survival Analysis}
In survival analysis \cite{clark2003survival}, I consider \(N\) observations each represented by triplets \(D = (x_i, t_i, d_i)\). Each \(x_i\) is an L-dimensional vector of covariates, \(t_i\) is the time-to-event, and \(d_i\) is a censor indicator where 0 denotes censored data and 1 denotes an observed event. SA models employ these covariates to define the probability density function \(p(t|x)\), the hazard rate \(h(t|x)\), and the survival function \(S(t|x) = 1 - F(t|x)\), where \(F(t|x)\) is the cumulative distribution function. These functions collectively form the foundation for modeling time-to-event data.


\begin{equation}
p(t|x) = h(t|x)S(t|x).
\end{equation}

\subsection{SAVAE-ELBO}
In our approach, I use a VAE (Variational Autoencoder) architecture to expand on the ELBO (Evidence Lower Bound) concept from standard VAE models. Our model, SAVAE, operates under the assumption that the two data-generating models,  \( p_{\theta_1} (x|z) \) and \( p_{\theta_2} (t|z) \),  are independent when \( z \) is known. This independence means that with knowledge of \( z \), I can generate either \( x \) or \( t \) separately. We do not allow \( Z \) to learn from \( t \) because \( t \) is not always available for all patients due to censorship. Furthermore, the VAE setup implies that each element within the data vector \( x \) is also independent from the others, provided \( z \) is given.

\begin{equation}
    p(x, t, z) = p_{\theta_1}(x \mid z) p_{\theta_2}(t \mid z) p(z) = p_{\theta}(x, t \mid z) p(z) 
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=4cm]{figures/SAVAE.png}
    \caption{The SAVAE Bayesian model is depicted with a graphical representation where the shadowed circle represents the latent variable, and the white circles denote the observables. In this model, the probabilities \(p_{\theta_1}(x|z)\) and \(p_{\theta_2}(t|z)\) represent the generative models, while \(q_\phi(z|x)\) is the variational approximation to the posterior. This approximation is necessary because the true posterior \(p(z|x)\) is unknown.
}
    \label{SAVAE}
\end{figure}

The model assumes that I know the distribution types of \( p_{\theta_1} (x \mid z) \) and \( p_{\theta_2} (t \mid z) \), but not the specific parameters \( \theta_1 \) and \( \theta_2 \). Given these assumptions, 
I compute the ELBO (Evidence Lower Bound) in a manner similar to the standard VAE. First, the conditional likelihood of a set of points \(\{x_i, t_i\}_{i=1}^N\) can be expressed as follows:


\[
\log p_{\theta}(x_1, x_2, \ldots, x_N , t_1, t_2, \ldots, t_N \mid z) = \sum_{i=1}^N \log p_{\theta}(x_i, t_i \mid z)
\]

\begin{equation}
= \sum_{i=1}^N \left(\log p_{\theta_2}(t_i \mid z) + \sum_{l=1}^L \log p_{\theta_1}(x_i^l \mid z)\right) 
\end{equation}

where the expected conditional likelihood can be expressed as:

\[
E_z \left[ p_\theta (x, t \mid z) \right] = \int p_\theta (x, t \mid z) p(z) \, dz = \int \frac{p_\theta(x,t,z)}{p(z)} p(z) \, dz
\]
\begin{equation}
=\int p_\theta(x,t,z) \, dz = p_\theta(x,t) = \int p_\theta(x,t,z) \cdot \frac{q_\phi(z|x)}{q_\phi(z|x)} \, dz 
\end{equation}
\[
= \mathbb{E}_{q_\phi(z|x)} \left[ \frac{p_\theta(x,t,z)}{q_\phi(z|x)} \right]
\]

As the interest lies in computing the log-likelihood:
\begin{equation}
\log p_\theta(x, t) = \log \left[ \mathbb{E}_{q_\phi(z|x)} \left[ \frac{p_\theta(x,t,z)}{q_\phi(z|x)} \right] \right]
\end{equation}
\[
 \geq \mathbb{E}_{q_\phi(z|x)} \left[ \log \left( \frac{p_\theta(x,t,z)}{q_\phi(z|x)} \right) \right],
\]
where the inequality comes from applying Jensen’s inequality.
The modeling choice for \( p_\theta(x, t, z) \) is defined as \( p_{\theta_1}(x|z) p_{\theta_2}(t|z) p(z) \), following the structure of a directed graphical model. This approach simplifies the joint probability distribution by leveraging conditional probabilities and assumed independences. Then, this could be rearranged as:

\[
\mathbb{E}_{q_\phi(z|x)} \left[ \log \left( \frac{p_\theta(x, t, z)}{q_\phi(z|x)} \right) \right] = \int q_\phi(z|x) \log \left( \frac{p_{\theta_1}(x|z) p_{\theta_2}(t|z) p(z)}{q_\phi(z|x)} \right) dz
\]

\[
= -\int q_\phi(z|x) \log \left( \frac{q_\phi(z|x)}{p(z)} \right) dz + \int q_\phi(z|x) \left[ \log p_{\theta_1}(x|z) + \log p_{\theta_2}(t|z) \right] dz
\]
\begin{equation}
= -D_{\text{KL}}(q_\phi(z|x) \parallel p(z)) + \mathbb{E}_{q_\phi(z|x)} \left[ \log p_{\theta_1}(x|z) + \log p_{\theta_2}(t|z) \right]
\end{equation}
\[
= L(x, \theta_1, \theta_2, \phi)
\]
After computing this ELBO, it can be seen that it is similar to the Vanilla VAE’s one (Equation 1). The only difference lies in the reconstruction term, which is expressed differently in order to explicitly distinguish between the covariates and the time-to-event.
\[
L(x, \theta_1, \theta_2, \phi)=
\]
\begin{equation}
\frac{1}{N} \sum_{i=1}^N \left( -D_{KL}(q_\phi(z|x_i) \parallel p(z)) + \log p_{\theta_2}(t_i|g_\phi(x_i, \epsilon_i)) + \sum_{l=1}^L \log p_{\theta_1}(x_{li}|g_\phi(x_i, \epsilon_i)) \right).
\end{equation}
In terms of implementation, three DNNs have been used. Note that the decoder DNNs output the parameters of each distribution.

\subsubsection{Divergence computation}

SAVAE assumes that \( q_\phi(z|x) \) follows a multidimensional Gaussian distribution, characterized by a vector of means \( \mu \), with each element denoted as \( \mu_j \), and by a diagonal covariance matrix \( C \). The main diagonal of \( C \) comprises the variances \( \sigma_j^2 \). This can be stated as:

\begin{equation}
-D_{KL}(q_\phi(z|x) \parallel p(z)) = \frac{1}{2} \sum_{j=1}^J \left(1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2\right),
\end{equation}

where \(J\) is the dimension of the latent space \(z\). This means that the Kullback-Leibler divergence from the ELBO, referred to as Equation 8 can be calculated analytically.

\subsubsection{Time modelling}
Handling survival data is challenged by censorship, where true survival times remain unknown, leading to incomplete observations. The SAVAE model addresses this by incorporating both censored (\(d_i=0\)) and uncensored (\(d_i=1\)) data to enhance parameter estimation accuracy. This integration is captured through the model's time probability density function (pdf), expressed as:

\begin{equation}
p_{\theta_2} (t_i|g_\phi(x_i, \epsilon_i)) = h(t_i|g_\phi(x_i, \epsilon_i))^{d_i} S(t_i|g_\phi(x_i, \epsilon_i)).
\end{equation}

This equation adapts the hazard and survival functions based on the censorship indicator \(d_i\), improving the model's reliability across different data scenarios.

The SAVAE model primarily employs the Weibull distribution for time-to-event data, which supports varying hazard functions and allows flexibility in the choice of distribution. The Weibull distribution is defined as follows:

\begin{equation}
p(t; \alpha, \lambda) = \frac{\alpha}{\lambda} \left(\frac{t}{\lambda}\right)^{\alpha-1} \exp\left(-\left(\frac{t}{\lambda}\right)^\alpha\right),
\end{equation}
\begin{equation}
S(t; \alpha, \lambda) = \exp\left(-\left(\frac{t}{\lambda}\right)^\alpha\right),
\end{equation}
\begin{equation}
h(t; \alpha, \lambda) = \frac{\alpha}{\lambda} \left(\frac{t}{\lambda}\right)^{\alpha-1}.
\end{equation}

These equations provide the basis for integrating different distributions, such as the exponential distribution (a special case of the Weibull with \(\alpha = 1\)), into the SAVAE model, demonstrating its versatility and capacity to predict the distribution parameters for each patient and calculate various statistics like means, medians, and percentiles.

\subsubsection{Marginal log-likelihood computation}
In the SAVAE model, variable types dictate the choice of distribution: Gaussian distributions are used for real-numbered variables due to their symmetry and continuous nature; Bernoulli distributions are selected for binary variables, representing the probability of binary outcomes; and Categorical distributions are employed for discrete variables with multiple categories, assigning probabilities to each possible outcome.

\subsection{Training Details}
The SAVAE model was implemented using PyTorch and involves three DNNs: one encoder and two decoders, focusing on covariates and time parameters. The encoder features a simple Gaussian structure with a single hidden layer using ReLU and an output layer with tanh activations, outputting to a fixed five-dimensional latent space. This space feeds into two decoders, each with two layers, the first with ReLU and dropout, and the final tailored to specific distribution parameters. Training spanned 3000 epochs with batch sizes of 64, including an Early Stop mechanism for efficiency.

The robustness of results was ensured through five-fold cross-validation, compared against models like Cox-PH, DeepHit, and DeepSurv, implemented via Pycox. Training variability was addressed by averaging the C-index across the best three out of ten random seed runs to ensure reliable performance metrics. Additional evaluations employed manual calculations and tools from the SciPy package.

\begin{figure}[H]
    \centering
    \includegraphics[width=13cm]{figures/DNN.png}
    \caption{SAVAE implementation using DNNs. One of them acts as an encoder which has the covariates vector as input. The other two act as decoders, one for the covariates and the other one for the time. }
    \label{DNN}

\end{figure}
\subsection{Evaluation metrics}
In Survival Analysis, datasets are comprised of \(N\) triplets \(D = (x_i, t_i, d_i)\), where \(x_i\) is an \(L\)-dimensional vector of covariates, \(t_i\) represents the time to event, and \(d_i\) serves as the censoring indicator. The primary evaluation metric, the C-index, gauges the correlation between predicted risks and observed event times, and is adapted for time-varying risks through the time-dependent C-index:


\begin{equation}
C_{\text{index}} = P(\hat{F}(t|x_i) > \hat{F}(t|x_j) | d_i = 1, t_i < t_j, t_i \leq t),
\end{equation}

Further, the Brier Score (BS) quantifies prediction errors, adjusted for censored data using Inverse Probability of Censoring Weighting (IPCW), which assesses both calibration and discrimination capabilities of models. The Integrated Brier Score (IBS) aggregates this assessment over all possible times, providing a comprehensive measure of model performance:


\begin{equation}
IBS(t_{\text{max}}) = \int_0^{t_{\text{max}}} BS(t) \, dt.
\end{equation}

These metrics collectively enhance the robust evaluation of SA models, incorporating accuracy, calibration, and adaptability to temporal changes in risk.

\section{Results}
This section assesses the performance of the SAVAE model relative to established models such as Cox-PH, DeepSurv, and DeepHit. Experiments across diverse medical datasets demonstrate the models' capability in predicting survival outcomes, focusing on both censored and uncensored data. Figure 4 and 5 illustrating that SAVAE's performance aligns closely with that of state-of-the-art models, evidenced by overlapping intervals of performance metrics across different data subsets.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{figures/c-indices.png}
    \caption{C-index average results across different folds for each state-of-the-art model. Average C-index results across the three best seeds for each fold in SAVAE performance.Bold highlights the best mean. For C-index higher is better }
    \label{c-index}

\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{figures/ibs.png}
    \caption{Ibs average results across different folds for each state-of-the-art model. Average Ibs results across the three best seeds for each fold in SAVAE performance.Bold highlights the best mean. For IBS lower is better  }
    \label{ibs}

\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=13cm]{figures/loss.png}
    \caption{Top Right: Kl loss, Top Left: Covariate loss, Bottom Right: Time loss, Bottom Left: Total loss }
    \label{loss}

\end{figure}


\section{Discussion}
The SAVAE model has shown promising potential for survival prediction; however, it underperforms across many datasets. Several factors might contribute to this issue:

\begin{itemize}
    \item \textbf{Decoder Dominance:} The model's decoder might overpower the KL divergence, leading to poor generalization on new data by focusing too much on reconstruction accuracy instead of forming meaningful latent representations.
    \item \textbf{Censored Data Handling:} The model struggles with censored data, potentially leading to biased predictions due to inadequate architectural and loss function adjustments.
    \item \textbf{Latent Space Limitations:} The continuous latent space may not effectively represent discrete or categorical variables that are crucial for accurate survival analysis.
    \item \textbf{Suboptimal Architecture:} The initial configuration of the model's architecture and parameters might not be well-suited for the data, necessitating a reassessment to enhance performance.
\end{itemize}

\textbf{Attempt with \(\beta\)-VAE:}

Despite adjustments, such as experimenting with \(\beta\)-VAE by varying the \(\beta\) parameter to values of 2, 10, and 50, there was no noticeable improvement in results. This outcome suggests that merely modifying \(\beta\) values might not resolve the fundamental challenges posed by the data characteristics or model configuration.

\textbf{Future Potential of SAVAE:}

Despite these challenges, SAVAE holds promise for adaptability and scalability, capable of integrating any parametric distribution with differentiable functions. Innovations in handling censoring reduce bias, enhancing reliability. Future applications could include clustering, data imputation, and synthetic data generation, broadening SAVAE’s utility in advanced analytics scenarios.







\bibliographystyle{plain}  
\bibliography{bibl.bib}
\cleardoublepage





\end{document}